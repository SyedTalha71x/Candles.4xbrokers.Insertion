import pkg, { Pool as PoolType } from "pg";
const { Pool } = pkg;
import { configDotenv } from "dotenv";
import candlesLogger from "./logger";
import { Queue } from 'bullmq';
import IORedis from 'ioredis';
import moment from "moment";

configDotenv();

const redisConnection = new IORedis({
  host: process.env.REDIS_HOST,
  port: process.env.REDIS_PORT ? Number(process.env.REDIS_PORT) : 6379,
  maxRetriesPerRequest: null
});

async function cleanQueues() {
  try {
    const tickQueue = new Queue('tickQueue', { connection: redisConnection });
    const candleQueue = new Queue('candleQueue', { connection: redisConnection });

    await tickQueue.obliterate({ force: true });
    await candleQueue.obliterate({ force: true });

    candlesLogger.info("Cleaned existing queues");
  } catch (error) {
    candlesLogger.error("Error cleaning queues:", error);
  }
}

export const tickQueue = new Queue('tickQueue', {
  connection: redisConnection,
  defaultJobOptions: {
    removeOnComplete: true,
    removeOnFail: 1000,
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 1000
    }
  }
});

export const candleQueue = new Queue('candleQueue', {
  connection: redisConnection,
  defaultJobOptions: {
    removeOnComplete: true,
    removeOnFail: 1000,
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 1000
    }
  }
});

// Database configuration
const CENTROID_HOST = process.env.CENTROID_PG_HOST;
const CENTROID_PORT = process.env.CENTROID_PG_PORT ? Number(process.env.CENTROID_PG_PORT) : 5432;
const CENTROID_USER = process.env.CENTROID_PG_USER;
const CENTROID_PASSWORD = process.env.CENTROID_PG_PASSWORD;
const CENTROID_DATABASE = process.env.CENTROID_PG_DATABASE;

const LPPIME_HOST = process.env.LPPRIME_PG_HOST;
const LPPIME_PORT = process.env.LPPRIME_PG_PORT ? Number(process.env.LPPRIME_PG_PORT) : 5432;
const LPPIME_USER = process.env.LPPRIME_PG_USER;
const LPPIME_PASSWORD = process.env.LPPRIME_PG_PASSWORD;
const LPPIME_DATABASE = process.env.LPPRIME_PG_DATABASE;

let isShuttingDown = false;

interface TickData {
  symbol: string;
  price: number;
  timestamp: Date;
  lots: number;
  type: "BID" | "ASK";
}

interface CurrencyPairInfo {
  currpair: string;
  contractsize: number | null;
}

interface TickBatch {
  data: TickData[];
  lastUpdated: moment.Moment;
}

interface OrganizedBatches {
  [key: string]: {
    BID: TickBatch;
    ASK: TickBatch;
  };
}

const centroidPool = new Pool({
  host: CENTROID_HOST,
  port: CENTROID_PORT,
  user: CENTROID_USER,
  password: CENTROID_PASSWORD,
  database: CENTROID_DATABASE,
  max: 30,
  idleTimeoutMillis: 60000,
  connectionTimeoutMillis: 60000,
  allowExitOnIdle: true,
  maxUses: 1000,
});

const lppimePool = new Pool({
  host: LPPIME_HOST,
  port: LPPIME_PORT,
  user: LPPIME_USER,
  password: LPPIME_PASSWORD,
  database: LPPIME_DATABASE,
  max: 30,
  idleTimeoutMillis: 60000,
  connectionTimeoutMillis: 60000,
  allowExitOnIdle: true,
  maxUses: 1000,
});

let availableCurrencyPairs: CurrencyPairInfo[] = [];
const subscribedPairs: Set<string> = new Set();

const BATCH_SIZE = process.env.BATCH_SIZE ? Number(process.env.BATCH_SIZE) : 1000;
const BATCH_TIMEOUT_MIN = process.env.BATCH_TIMEOUT
  ? Number(process.env.BATCH_TIMEOUT) * 60 * 1000
  : 5 * 60 * 1000;

let organizedBatches: OrganizedBatches = {};
let cleanupInterval: NodeJS.Timeout | null = null;

async function cleanFailedJobs() {
  try {
    const failedTickJobs = await tickQueue.getFailed();
    const failedCandleJobs = await candleQueue.getFailed();

    if (failedTickJobs.length > 0 || failedCandleJobs.length > 0) {
      candlesLogger.info(`Cleaning ${failedTickJobs.length} failed tick jobs and ${failedCandleJobs.length} failed candle jobs`);
      await Promise.all([
        ...failedTickJobs.map(job => job.remove()),
        ...failedCandleJobs.map(job => job.remove())
      ]);
    }
  } catch (error) {
    candlesLogger.error('Error during queue cleanup:', error);
  }
}

async function processTickBatch(symbol: string, bidTicks: TickData[], askTicks: TickData[]) {
  if (bidTicks.length === 0 && askTicks.length === 0) return;

  candlesLogger.debug(`Processing tick batch for ${symbol}`, {
    bidCount: bidTicks.length,
    askCount: askTicks.length,
    firstBidTimestamp: bidTicks[0]?.timestamp.toISOString(),
    lastBidTimestamp: bidTicks[bidTicks.length - 1]?.timestamp.toISOString(),
    firstAskTimestamp: askTicks[0]?.timestamp.toISOString(),
    lastAskTimestamp: askTicks[askTicks.length - 1]?.timestamp.toISOString()
  });

  try {
    await tickQueue.add('processTickBatch', { symbol, bidTicks, askTicks });
    candlesLogger.debug(`Enqueued tick batch for ${symbol}`, {
      bidCount: bidTicks.length,
      askCount: askTicks.length
    });
  } catch (error) {
    candlesLogger.error(`Failed to enqueue tick batch for ${symbol}:`, error);
    throw error;
  }
}

async function processCandleBatch(symbol: string, bidTicks: TickData[]) {
  if (bidTicks.length === 0) return;

  candlesLogger.debug(`Processing candle batch for ${symbol}`, {
    tickCount: bidTicks.length,
    firstTimestamp: bidTicks[0]?.timestamp.toISOString(),
    lastTimestamp: bidTicks[bidTicks.length - 1]?.timestamp.toISOString()
  });

  try {
    await candleQueue.add('processCandleBatch', { symbol, bidTicks });
    candlesLogger.debug(`Enqueued candle batch for ${symbol}`, {
      tickCount: bidTicks.length
    });
  } catch (error) {
    candlesLogger.error(`Failed to enqueue candle batch for ${symbol}:`, error);
    throw error;
  }
}



async function pgListener() {
  let client;
  try {
    client = await centroidPool.connect();
    candlesLogger.info("Connected to Centroid PostgreSQL for tick notifications");

    client.on("notification", async (msg) => {
      if (msg.channel === 'tick') {
        try {
          const [symbol, lotsStr, bora, priceStr, timestampStr] = (msg.payload ?? "").split(" ");
          const lots = parseInt(lotsStr);
          const price = parseFloat(priceStr);
          const timestamp = new Date(parseInt(timestampStr));

          if (!timestamp.getTime() || isNaN(lots) || isNaN(price)) {
            candlesLogger.error("Invalid tick data received", {
              symbol,
              lots,
              price,
              timestamp: timestamp.getTime()
            });
            return;
          }

          const tickData: TickData = {
            symbol,
            price,
            timestamp,
            lots,
            type: bora === "A" ? "ASK" : "BID"
          };

          // Initialize batch if not exists
          if (!organizedBatches[symbol]) {
            organizedBatches[symbol] = {
              BID: { data: [], lastUpdated: moment() },
              ASK: { data: [], lastUpdated: moment() }
            };
          }

          // Add to appropriate batch
          const batchType = tickData.type;
          const batch = organizedBatches[symbol][batchType];
          batch.data.push(tickData);
          // console.log(batch.data.length)
          // candlesLogger.info(` batch length ---- ${batch.data.length} ${symbol} ${batchType}`)

          // Check if batch size is reached (priority condition)
          if (batch.data.length === BATCH_SIZE) {
            // console.log(batch.data.length === BATCH_SIZE);
            // candlesLogger.info(`batch size lenght condition is true ${batch.data.length === BATCH_SIZE}`)
            
            const ticksToProcess = batch.data.splice(0, BATCH_SIZE);
            batch.lastUpdated = moment();

            if (batchType === "BID") {
              // Process tick and candle batches in parallel
              await Promise.all([
                processTickBatch(symbol, ticksToProcess, []),
                processCandleBatch(symbol, ticksToProcess)
              ]);
            } else {
              await processTickBatch(symbol, [], ticksToProcess);
            }
          }
          // Only check timeout if batch not processed by size
          else {
            const now = moment();
            const timeSinceLastProcess = now.diff(batch.lastUpdated, 'milliseconds');
            if (timeSinceLastProcess >= BATCH_TIMEOUT_MIN && batch.data.length > 1) {
              // console.log(timeSinceLastProcess >= BATCH_TIMEOUT_MIN && batch.data.length > 1);
              // candlesLogger.info(`Condition inside timeout --- ${timeSinceLastProcess >= BATCH_TIMEOUT_MIN && batch.data.length > 1}`)
              

              const ticksToProcess = batch.data.splice(0, batch.data.length - 1);
              
              // Don't update lastUpdated until after actual processing
              if (ticksToProcess.length > 0) {
                batch.lastUpdated = moment();
          
                if (batchType === "BID") {
                  await Promise.all([
                    processTickBatch(symbol, ticksToProcess, []),
                    processCandleBatch(symbol, ticksToProcess)
                  ]);
                } else {
                  await processTickBatch(symbol, [], ticksToProcess);
                }
              }
            }
          }

        } catch (error: any) {
          candlesLogger.error("Error processing tick notification:", error);
        }
      }
    });

    client.on('error', (err) => {
      candlesLogger.error("Centroid client error:", err);
    });

    await client.query("LISTEN tick");
    candlesLogger.info("Now listening for tick notifications");

  } catch (error: any) {
    candlesLogger.error(`Centroid connection failed: ${error.message}`);
    throw error;
  }
}

async function initDatabase() {
  try {
    await fetchAllCurrencyPairs();
    return true;
  } catch (error) {
    candlesLogger.error("LPPime database init failed:", error);
    throw error;
  }
}

async function fetchAllCurrencyPairs() {
  let client;
  try {
    client = await centroidPool.connect();
    const result = await client.query(
      "SELECT currpair, contractsize FROM currpairdetails"
    );
    availableCurrencyPairs = result.rows;
    candlesLogger.info(`Found ${availableCurrencyPairs.length} currency pairs in Centroid`);

    const validPairs = availableCurrencyPairs.filter(
      (pair) => pair.contractsize !== null
    );
    candlesLogger.info(`Found ${validPairs.length} valid pairs with contract size`);

    for (const pair of validPairs) {
      subscribedPairs.add(pair.currpair);
      const tableNameBid = `ticks_${pair.currpair.toLowerCase()}_bid`;
      const tableNameAsk = `ticks_${pair.currpair.toLowerCase()}_ask`;
      const candleTableName = `candles_${pair.currpair.toLowerCase()}_bid`;

      // Create tables in both databases
      await Promise.all([
        ensureTableExists(centroidPool, tableNameBid),
        ensureTableExists(centroidPool, tableNameAsk),
        ensureCandleTableExists(centroidPool, candleTableName),
        ensureTableExists(lppimePool, tableNameBid),
        ensureTableExists(lppimePool, tableNameAsk),
        ensureCandleTableExists(lppimePool, candleTableName)
      ]);
    }
  } catch (error) {
    candlesLogger.error("Error initializing database tables:", error);
    throw error;
  } finally {
    if (client) await client.release();
  }
}

async function ensureTableExists(pool: PoolType, tableName: string): Promise<void> {
  let client;
  try {
    client = await pool.connect();
    const exists = await client.query(
      `SELECT EXISTS (SELECT FROM information_schema.tables 
       WHERE table_schema = 'public' AND table_name = $1)`,
      [tableName]
    );

    if (!exists.rows[0].exists) {
      await client.query(`
        CREATE TABLE ${tableName} (
          ticktime TIMESTAMP WITH TIME ZONE NOT NULL,
          lots INTEGER NOT NULL,
          price NUMERIC NOT NULL,
          PRIMARY KEY (lots, ticktime)
        )`);
      candlesLogger.info(`Created table ${tableName} in ${pool === centroidPool ? 'Centroid' : 'LPPime'}`);
    }
  } catch (error) {
    candlesLogger.error(`Error ensuring table ${tableName} exists in ${pool === centroidPool ? 'Centroid' : 'LPPime'}:`, error);
    throw error;
  } finally {
    if (client) await client.release();
  }
}


async function ensureCandleTableExists(pool: PoolType, tableName: string): Promise<void> {
  let client;
  try {
    client = await pool.connect();
    const exists = await client.query(
      `SELECT EXISTS (SELECT FROM information_schema.tables 
       WHERE table_schema = 'public' AND table_name = $1)`,
      [tableName]
    );

    if (!exists.rows[0].exists) {
      await client.query(`
        CREATE TABLE ${tableName} (
          candlesize TEXT NOT NULL,
          lots SMALLINT,
          candletime TIMESTAMP WITH TIME ZONE NOT NULL,
          open NUMERIC(12,5) NOT NULL,
          high NUMERIC(12,5) NOT NULL,
          low NUMERIC(12,5) NOT NULL,
          close NUMERIC(12,5) NOT NULL,
          PRIMARY KEY (candlesize, lots, candletime)
        )`);
      candlesLogger.info(`Created candle table ${tableName} in ${pool === centroidPool ? 'Centroid' : 'LPPime'}`);
    }
  } catch (error) {
    candlesLogger.error(`Error ensuring candle table ${tableName} exists in ${pool === centroidPool ? 'Centroid' : 'LPPime'}:`, error);
    throw error;
  } finally {
    if (client) await client.release();
  }
}

async function initializeApplication() {
  try {
    candlesLogger.info('Starting Candles Server...');
    await cleanQueues();

    organizedBatches = {};

    // clean every failed jobs after every 10 minutes
    cleanupInterval = setInterval(cleanFailedJobs, 10 * 60 * 1000);

    await cleanFailedJobs();

    await initDatabase();
    await pgListener();
    candlesLogger.info('Application initialized. Waiting for ticks...');
  } catch (error) {
    candlesLogger.error("Application initialization failed:", error);
    process.exit(1);
  }
}

process.on("SIGINT", async () => {
  if (isShuttingDown) return;
  isShuttingDown = true;
  candlesLogger.info("Shutting down gracefully...");

  try {

    await tickQueue.close();
    await candleQueue.close();
    await redisConnection.quit();
    await centroidPool.end();
    candlesLogger.info("Resources closed successfully");
    process.exit(0);
  } catch (error) {
    candlesLogger.error("Error during shutdown:", error);
    process.exit(1);
  }
});

initializeApplication();



-----------------------------------------------


import { Worker } from 'bullmq';
import IORedis from 'ioredis';
import pkg, { PoolClient } from "pg";
const { Pool } = pkg;
import { configDotenv } from "dotenv";
import candlesLogger from "./logger";

configDotenv();

const redisConnection = new IORedis({
  host: process.env.REDIS_HOST,
  port: process.env.REDIS_PORT ? Number(process.env.REDIS_PORT) : 6379,
  maxRetriesPerRequest: null
});

const centroidPool = new Pool({
  host: process.env.CENTROID_PG_HOST,
  port: process.env.CENTROID_PG_PORT ? Number(process.env.CENTROID_PG_PORT) : 5432,
  user: process.env.CENTROID_PG_USER,
  password: process.env.CENTROID_PG_PASSWORD,
  database: process.env.CENTROID_PG_DATABASE,
  max: 30,
  idleTimeoutMillis: 60000,
  connectionTimeoutMillis: 60000,
  allowExitOnIdle: true,
  maxUses: 1000,
});

const lppimePool = new Pool({
  host: process.env.LPPIME_HOST,
  port: process.env.LPPIME_PORT ? Number(process.env.LPPIME_PORT) : 5432,
  user: process.env.LPPIME_USER,
  password: process.env.LPPIME_PASSWORD,
  database: process.env.LPPIME_DATABASE,
  max: 30,
  idleTimeoutMillis: 60000,
  connectionTimeoutMillis: 60000,
  allowExitOnIdle: true,
  maxUses: 1000,
});

interface TickData {
  symbol: string;
  price: number;
  timestamp: Date;
  lots: number;
  type: "BID" | "ASK";
}

const timeFrames: Record<string, number> = {
  M1: 60000,
  H1: 3600000,
  D1: 86400000,
};

const tickWorker = new Worker('tickQueue', async (job) => {
  if (job.name === 'processTickBatch') {
    const { symbol, bidTicks, askTicks } = job.data;
    const centroidClient = await centroidPool.connect();
    const lppimeClient = await lppimePool.connect();

    try {
      // Begin transactions in both databases
      await Promise.all([
        centroidClient.query('BEGIN'),
        lppimeClient.query('BEGIN')
      ]);
      
      if (bidTicks.length > 0) {
        await Promise.all([
          storeTickDataBatch(symbol, "bid", bidTicks, centroidClient, 'centroid'),
          storeTickDataBatch(symbol, "bid", bidTicks, lppimeClient, 'lppime')
        ]);
      }
      
      if (askTicks.length > 0) {
        await Promise.all([
          storeTickDataBatch(symbol, "ask", askTicks, centroidClient, 'centroid'),
          storeTickDataBatch(symbol, "ask", askTicks, lppimeClient, 'lppime')
        ]);
      }
      
      // Commit transactions in both databases
      await Promise.all([
        centroidClient.query('COMMIT'),
        lppimeClient.query('COMMIT')
      ]);
      
      candlesLogger.info(`Successfully processed tick batch for ${symbol} (BID: ${bidTicks.length}, ASK: ${askTicks.length}) in both databases`);
      
    } catch (error) {
      // Rollback transactions in both databases if any error occurs
      await Promise.allSettled([
        centroidClient.query('ROLLBACK').catch(e => candlesLogger.error('Centroid rollback error:', e)),
        lppimeClient.query('ROLLBACK').catch(e => candlesLogger.error('Lppime rollback error:', e))
      ]);
      throw error;
    } finally {
      centroidClient.release();
      lppimeClient.release();
    }
  }
}, { 
  connection: redisConnection,
  concurrency: 3,
  limiter: { max: 10, duration: 1000 }
});

async function storeTickDataBatch(symbol: string, type: "bid" | "ask", ticks: TickData[], client: PoolClient, dbName: string) {
  try {
    const tableName = `ticks_${symbol.toLowerCase()}_${type}`;
    
    candlesLogger.info(`Processing ${ticks.length} ticks for ${symbol} ${type} in ${dbName}`);
    
    if (ticks.length > 0) {
      candlesLogger.debug(`Sample tick format in ${dbName}: ${JSON.stringify(ticks[0])}`);
    }
    
    // Ensure all timestamps are Date objects
    const processedTicks = ticks.map(tick => {
      if (!(tick.timestamp instanceof Date)) {
        return {
          ...tick,
          timestamp: new Date(tick.timestamp)
        };
      }
      return tick;
    });
    
    // Check existing ticks with all three values (timestamp, lots, AND price)
    const existingTicksQuery = await client.query({
      text: `
        SELECT ticktime, lots, price FROM ${tableName}
        WHERE (ticktime, lots, price) IN (
          ${processedTicks.map((_, i) => `($${i*3+1}, $${i*3+2}, $${i*3+3})`).join(', ')}
        )
      `,
      values: processedTicks.flatMap(tick => {
        try {
          return [
            tick.timestamp.toISOString(),
            tick.lots,
            tick.price
          ];
        } catch (err) {
          if (err instanceof Error) {
            candlesLogger.error(`Error in ${dbName} converting timestamp for tick: ${JSON.stringify(tick)}, Error: ${err.message}`);
          } else {
            candlesLogger.error(`Error in ${dbName} converting timestamp for tick: ${JSON.stringify(tick)}, Error: ${String(err)}`);
          }
          throw err;
        }
      })
    });

    const existingTicksMap = new Map<string, boolean>();
    for (const row of existingTicksQuery.rows) {
      const key = `${row.ticktime.toISOString()}_${row.lots}_${row.price}`;
      existingTicksMap.set(key, true);
    }

    const ticksToProcess = processedTicks.filter(tick => {
      const key = `${tick.timestamp.toISOString()}_${tick.lots}_${tick.price}`;
      return !existingTicksMap.has(key);
    });

    if (ticksToProcess.length === 0) {
      candlesLogger.info(`All ${ticks.length} ticks already exist with identical values in ${dbName}`);
      return;
    }

    const values = [];
    const params = [];
    let paramIndex = 1;
    
    for (const tick of ticksToProcess) {
      values.push(`($${paramIndex++}, $${paramIndex++}, $${paramIndex++})`);
      params.push(
        tick.timestamp.toISOString(),
        tick.lots,
        tick.price
      );
    }
    
    const queryText = `
      INSERT INTO ${tableName} (ticktime, lots, price)
      VALUES ${values.join(', ')}
    `;
    
    const result = await client.query(queryText, params);
    candlesLogger.info(`Successfully inserted ${result.rowCount} ticks into ${tableName} in ${dbName}`);
    
  } catch (error) {
    candlesLogger.error(`Error in ${dbName} batch processing for ${symbol} ${type}:`, error);
    throw error;
  }
}

const candleWorker = new Worker('candleQueue', async (job) => {
  if (job.name === 'processCandleBatch') {
    const { symbol, bidTicks } = job.data;
    const centroidClient = await centroidPool.connect();
    const lppimeClient = await lppimePool.connect();

    try {
      // Begin transactions in both databases
      await Promise.all([
        centroidClient.query('BEGIN'),
        lppimeClient.query('BEGIN')
      ]);
      
      if (bidTicks.length > 0) {
        candlesLogger.info(`Processing ${bidTicks.length} ticks for ${symbol} candles in both databases`);
        await Promise.all([
          processCandleBatch(symbol, bidTicks, centroidClient, 'centroid'),
          processCandleBatch(symbol, bidTicks, lppimeClient, 'lppime')
        ]);
      }
      
      // Commit transactions in both databases
      await Promise.all([
        centroidClient.query('COMMIT'),
        lppimeClient.query('COMMIT')
      ]);
      
      candlesLogger.info(`Successfully processed candle batch for ${symbol} with ${bidTicks.length} ticks in both databases`);

    } catch (error) {
      // Rollback transactions in both databases if any error occurs
      await Promise.allSettled([
        centroidClient.query('ROLLBACK').catch(e => candlesLogger.error('Centroid rollback error:', e)),
        lppimeClient.query('ROLLBACK').catch(e => candlesLogger.error('Lppime rollback error:', e))
      ]);
      throw error;
    } finally {
      centroidClient.release();
      lppimeClient.release();
    }
  }
}, { connection: redisConnection, concurrency: 3 });

async function processCandleBatch(symbol: string, ticks: TickData[], client: PoolClient, dbName: string) {
  try {
    const tableName = `candles_${symbol.toLowerCase()}_bid`;
    const lots = 1;
    
    candlesLogger.info(`Processing ${ticks.length} candle ticks for ${symbol} in ${dbName}`);
    
    if (ticks.length > 0) {
      candlesLogger.debug(`Sample candle tick format in ${dbName}: ${JSON.stringify(ticks[0])}`);
    }
    
    // Ensure all timestamps are Date objects
    const processedTicks = ticks.map(tick => {
      if (!(tick.timestamp instanceof Date)) {
        return {
          ...tick,
          timestamp: new Date(tick.timestamp)
        };
      }
      return tick;
    });
    
    for (const timeframe of Object.keys(timeFrames)) {
      const duration = timeFrames[timeframe];
      const candleMap = new Map<string, {
        time: Date,
        minPrice: number,
        maxPrice: number,
        firstPrice: number,
        lastPrice: number
      }>();
      
      // Group ticks into candles
      for (const tick of processedTicks) {
        try {
          const candleTime = new Date(Math.floor(tick.timestamp.getTime() / duration) * duration);
          const candleTimeStr = candleTime.toISOString();
          
          if (!candleMap.has(candleTimeStr)) {
            candleMap.set(candleTimeStr, {
              time: candleTime,
              minPrice: tick.price,
              maxPrice: tick.price,
              firstPrice: tick.price,
              lastPrice: tick.price
            });
          } else {
            const candle = candleMap.get(candleTimeStr)!;
            candle.minPrice = Math.min(candle.minPrice, tick.price);
            candle.maxPrice = Math.max(candle.maxPrice, tick.price);
            candle.lastPrice = tick.price;
          }
        } catch (err) {
          candlesLogger.error(`Error in ${dbName} processing tick for candle`, {
            tick,
            error: err instanceof Error ? err.message : String(err)
          });
          continue;
        }
      }
      
      const candleTimes = Array.from(candleMap.keys());
      
      if (candleTimes.length === 0) {
        candlesLogger.debug(`No valid candles to process for ${symbol} ${timeframe} in ${dbName}`);
        continue;
      }
      
      // Check existing candles
      const existingQuery = await client.query({
        text: `
          SELECT candletime, open, high, low, close 
          FROM ${tableName} 
          WHERE candlesize = $1 AND lots = $2 AND candletime = ANY($3::timestamp[])
        `,
        values: [timeframe, lots, candleTimes]
      });
      
      const existingCandles = new Map<string, {
        open: number,
        high: number,
        low: number,
        close: number
      }>();
      
      for (const row of existingQuery.rows) {
        existingCandles.set(row.candletime.toISOString(), {
          open: parseFloat(row.open),
          high: parseFloat(row.high),
          low: parseFloat(row.low),
          close: parseFloat(row.close)
        });
      }
      
      // Prepare insert and update operations
      const insertCandles: Array<{
        time: Date,
        firstPrice: number,
        minPrice: number,
        maxPrice: number,
        lastPrice: number
      }> = [];
      
      const updateCandles: Array<{
        time: Date,
        minPrice: number,
        maxPrice: number,
        lastPrice: number
      }> = [];
      
      for (const [candleTimeStr, candle] of candleMap.entries()) {
        const existingCandle = existingCandles.get(candleTimeStr);
        
        if (!existingCandle) {
          insertCandles.push({
            time: candle.time,
            firstPrice: candle.firstPrice,
            minPrice: candle.minPrice,
            maxPrice: candle.maxPrice,
            lastPrice: candle.lastPrice
          });
        } else {
          const newMin = Math.min(existingCandle.low, candle.minPrice);
          const newMax = Math.max(existingCandle.high, candle.maxPrice);
          
          if (newMin !== existingCandle.low || newMax !== existingCandle.high || candle.lastPrice !== existingCandle.close) {
            updateCandles.push({
              time: candle.time,
              minPrice: newMin,
              maxPrice: newMax,
              lastPrice: candle.lastPrice
            });
          }
        }
      }
      
      // Process inserts in batches of 500
      const insertBatchSize = 500;
      for (let i = 0; i < insertCandles.length; i += insertBatchSize) {
        const batch = insertCandles.slice(i, i + insertBatchSize);
        const values = batch.map((_, idx) => 
          `($${idx*7+1}, $${idx*7+2}, $${idx*7+3}, $${idx*7+4}, $${idx*7+5}, $${idx*7+6}, $${idx*7+7})`
        ).join(', ');
        
        const params = batch.flatMap(c => [
          timeframe,
          lots,
          c.time.toISOString(),
          c.firstPrice,
          c.maxPrice,
          c.minPrice,
          c.lastPrice
        ]);
        
        await client.query({
          text: `
            INSERT INTO ${tableName} 
            (candlesize, lots, candletime, open, high, low, close)
            VALUES ${values}
          `,
          values: params
        });
      }
      
      candlesLogger.info(`Inserted ${insertCandles.length} new candles for ${symbol} ${timeframe} in ${dbName}`);
      
      // Process updates in batches of 100
      const updateBatchSize = 100;
      for (let i = 0; i < updateCandles.length; i += updateBatchSize) {
        const batch = updateCandles.slice(i, i + updateBatchSize);
        
        let paramIndex = 1;
        const timeValues = [];
        const caseStatements = {
          low: [] as string[],
          high: [] as string[],
          close: [] as string[]
        };
        const queryParams = [];
        
        for (const candle of batch) {
          timeValues.push(`$${paramIndex}`);
          queryParams.push(candle.time.toISOString());
          
          caseStatements.low.push(`WHEN candletime = $${paramIndex} THEN $${paramIndex+1}`);
          caseStatements.high.push(`WHEN candletime = $${paramIndex} THEN $${paramIndex+2}`);
          caseStatements.close.push(`WHEN candletime = $${paramIndex} THEN $${paramIndex+3}`);
          
          queryParams.push(candle.minPrice, candle.maxPrice, candle.lastPrice);
          paramIndex += 4;
        }
        
        queryParams.push(timeframe, lots);
        
        const updateQuery = `
          UPDATE ${tableName}
          SET
            low = CASE ${caseStatements.low.join(' ')} ELSE low END,
            high = CASE ${caseStatements.high.join(' ')} ELSE high END,
            close = CASE ${caseStatements.close.join(' ')} ELSE close END
          WHERE candlesize = $${paramIndex++}
            AND lots = $${paramIndex++}
            AND candletime IN (${timeValues.join(', ')})
        `;
        
        await client.query({
          text: updateQuery,
          values: queryParams
        });
      }
      
      candlesLogger.info(`Updated ${updateCandles.length} existing candles for ${symbol} ${timeframe} in ${dbName}`);
    }
  } catch (error) {
    candlesLogger.error(`Error in ${dbName} processing candle batch for ${symbol}:`, {
      error: error instanceof Error ? error.message : String(error),
      symbol,
      tickCount: ticks.length
    });
    throw error;
  }
}

process.on("SIGINT", async () => {
  candlesLogger.info("Shutting down worker gracefully...");
  try {
    await Promise.all([
      tickWorker.close(),
      candleWorker.close(),
      redisConnection.quit(),
      centroidPool.end(),
      lppimePool.end()
    ]);
    process.exit(0);
  } catch (error) {
    candlesLogger.error("Error during worker shutdown:", error);
    process.exit(1);
  }
});
